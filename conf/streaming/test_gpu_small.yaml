# Small test configuration for GPU validation
defaults:
  - _self_
  - tokenizer: google
  - model: 
    - neobert
  - optimizer: adamw
  - scheduler: cosine_decay
  - trainer: mlm
  - dataloader: base
  - datacollator: mlm_20

# Dataset configuration
dataset:
  path_to_disk: "tokenized_data/wikitext_small"

# Override model size for quick testing
model:
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 1024
  flash_attention: true

# Small dataset and training params
trainer:
  max_steps: 20
  gradient_accumulation_steps: 1
  mixed_precision: "bf16"
  dir: "outputs/test_gpu"
  save_steps: 10
  logging_steps: 5
  disable_tqdm: false
  eval_steps: 100
  
dataloader:
  train:
    batch_size: 8
    num_workers: 0

scheduler:
  warmup_steps: 5
  total_steps: 20
  final_lr_ratio: 0.1

optimizer:
  hparams:
    lr: 1e-4

wandb:
  resume: never
  name: null
  project: "neo-bert"
  entity: null
  tags: []
  dir: "logs/wandb"
  mode: disabled
  log_interval: 100

seed: 42