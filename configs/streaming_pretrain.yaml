dataset:
  name: "common-pile/comma_v0.1_training_dataset"  # Or any HuggingFace dataset
  train_split: "train"
  streaming: true
  shuffle_buffer_size: 10000  # Buffer size for shuffling streaming data
  num_workers: 0  # Must be 0 for streaming datasets to avoid pickling errors
  max_seq_length: 512

model:
  path: "google-bert/bert-base-uncased"  # Or your custom model
  vocab_size: 30522
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  max_position_embeddings: 512
  dropout_prob: 0.1
  use_flash_attention: true  # Set to false if GPU doesn't support it
  flash_attention: true
  rope: true  # RoPE embeddings
  rms_norm: true  # RMSNorm instead of LayerNorm

tokenizer:
  path: "google-bert/bert-base-uncased"
  max_length: 512
  padding: "max_length"
  truncation: true

trainer:
  max_steps: 1000000
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  save_steps: 10000
  eval_steps: 10000
  logging_steps: 100
  warmup_steps: 10000
  learning_rate: 5.0e-4
  weight_decay: 0.01
  adam_epsilon: 1.0e-8
  adam_beta1: 0.9
  adam_beta2: 0.999
  max_grad_norm: 1.0
  bf16: true
  mixed_precision: "bf16"
  gradient_checkpointing: false
  seed: 42
  output_dir: "./outputs/streaming_pretrain"
  overwrite_output_dir: true
  debug: false
  remove_unused_columns: true
  dataloader_pin_memory: true
  dataloader_num_workers: 0

dataloader:
  batch_size: 16
  shuffle: false  # Streaming datasets handle shuffling via buffer
  num_workers: 0
  pin_memory: true
  drop_last: true

datacollator:
  mlm_probability: 0.15
  pad_to_multiple_of: 8

optimizer:
  name: "adamw"
  lr: 5.0e-4
  weight_decay: 0.01
  eps: 1.0e-8
  betas: [0.9, 0.999]

scheduler:
  scheduler_type: "cosine"
  warmup_steps: 10000

wandb:
  enabled: true
  project: "neo-bert-streaming"
  name: "streaming-pretrain"
  log_model: false