task: "pretraining"

model:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.05
  vocab_size: 100096
  max_position_embeddings: 4096
  flash_attention: true
  attention_backend: "auto"
  ngpt: false
  hidden_act: "swiglu"  # Requires xformers
  rope: true
  rms_norm: true
  # Warm start from 1024-run
  pretrained_checkpoint_dir: "./outputs/smollm2_custom_tokenizer_1024"
  pretrained_checkpoint: "latest"

dataset:
  name: "EleutherAI/SmolLM2-1.7B-stage-4-100B"
  streaming: true  # Stream this large dataset
  num_proc: 8
  num_workers: 4
  max_seq_length: 2048
  train_split: "train"
  eval_split: "train[:1%]"  # Use small subset for eval

tokenizer:
  name: "amazingvince/superbpe_t100k_m10k"
  vocab_size: 100096
  max_length: 2048

datacollator:
  mlm_probability: 0.4
  pad_to_multiple_of: 8

trainer:
  output_dir: "./outputs/smollm2_custom_tokenizer_2048"
  run_name: "neoBERT-120m-smollm2-wordpiece_msp_tok"
  max_steps: 200000
  per_device_train_batch_size: 8
  per_device_eval_batch_size:  8
  gradient_accumulation_steps: 8
  learning_rate: 1e-4
  warmup_steps: 5000
  logging_steps: 25
  eval_steps: 5000
  save_steps: 10000
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: ["wandb"]
  bf16: true
  gradient_checkpointing: false
  resume_from_checkpoint: false

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1e-16

scheduler:
  name: "cosine"
  warmup_steps: 5000
  total_steps: 200000

wandb:
  project: "neobert-testing"
  name: "neoBERT-120m-smollm2-superbpe_t100k_m10k"
  mode: "online"

# Additional settings,
seed: 69
mixed_precision: "bf16"
