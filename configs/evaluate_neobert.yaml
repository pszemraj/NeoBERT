task: "glue"

# Model configuration - simplified
model:
  name_or_path: "bert-base-uncased"
  # Pretrained model to load
  pretrained_checkpoint_dir: "./outputs/neobert_100m_100k"
  pretrained_checkpoint: 100000
  pretrained_config_path: "./outputs/neobert_100m_100k/model_checkpoints/100000/config.yaml"
  # Model architecture (must match pretrained)
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 512
  hidden_act: "swiglu"
  # Classifier settings
  classifier_dropout: 0.1
  classifier_init_range: 0.02

# GLUE-specific configuration
glue:
  task_name: "cola"
  num_labels: 2
  max_seq_length: 128
  # Uncomment for testing with random weights
  # allow_random_weights: true

# Tokenizer
tokenizer:
  name: "bert-base-uncased"
  max_length: 128

# Training configuration
trainer:
  output_dir: "./glue_outputs/cola"
  num_train_epochs: 3
  max_steps: -1  # Auto-calculate from epochs
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  # Evaluation and saving
  eval_strategy: "steps"
  eval_steps: 10
  save_strategy: "steps"
  save_steps: 10
  save_total_limit: 3
  logging_steps: 50
  # Early stopping
  early_stopping: 3
  metric_for_best_model: "eval_matthews_correlation"
  greater_is_better: true
  load_best_model_at_end: true
  # Mixed precision
  mixed_precision: "bf16"
  tf32: true
  # Misc
  seed: 42
  report_to: []

# Optimizer
optimizer:
  name: "adamw"
  lr: 2e-5
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning rate scheduler
scheduler:
  name: "linear"
  warmup_percent: 10

# Data collator
datacollator:
  pad_to_multiple_of: 8

# Weights & Biases (disabled by default)
wandb:
  project: "neobert-glue"
  mode: "disabled"