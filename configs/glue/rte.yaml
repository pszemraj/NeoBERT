task: glue
model:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 512
  hidden_act: swiglu
  name: neobert-100m
glue:
  task_name: rte
  num_labels: 2
  max_seq_length: 512
  pretrained_model_path: ./outputs/neobert_100m_100k/model_checkpoints/100000/config.yaml
  pretrained_checkpoint_dir: ./outputs/neobert_100m_100k
  pretrained_checkpoint: 100000
  classifier_dropout: 0.1
  classifier_init_range: 0.02
  num_workers: 4
  preprocessing_num_proc: 4
tokenizer:
  name: bert-base-uncased
  max_length: 512
  truncation: true
trainer:
  output_dir: ./outputs/glue/neobert-100m/rte
  num_train_epochs: 3
  max_steps: -1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  eval_strategy: epoch
  eval_steps: 50
  save_model: false
  save_strategy: 'no'
  save_steps: 100
  save_total_limit: 0
  logging_steps: 100
  early_stopping: 5
  metric_for_best_model: eval_accuracy
  greater_is_better: true
  load_best_model_at_end: true
  mixed_precision: bf16
  tf32: true
optimizer:
  name: adamw
  lr: 2e-5
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.999
  eps: 1e-8
scheduler:
  name: linear
  warmup_percent: 10
datacollator:
  pad_to_multiple_of: 8
wandb:
  enabled: true
  project: neobert-glue
  name: neobert-100m-rte-100k
  mode: online
seed: 42
