task: glue
model:
  name_or_path: neobert-100m
  pretrained_checkpoint_dir: ./outputs/neobert_100m_100k
  pretrained_checkpoint: 100000
  pretrained_config_path: ./outputs/neobert_100m_100k/model_checkpoints/100000/config.yaml
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 512
  hidden_act: swiglu
  classifier_dropout: 0.1
  classifier_init_range: 0.02
glue:
  task_name: qnli
  num_labels: 2
  max_seq_length: 128
tokenizer:
  name: bert-base-uncased
  max_length: 128
trainer:
  output_dir: ./outputs/glue/neobert-100m/qnli
  num_train_epochs: 3
  max_steps: -1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  eval_strategy: steps
  eval_steps: 500
  save_strategy: steps
  save_steps: -1  # Disabled
  save_total_limit: 0  # No checkpoints
  logging_steps: 100
  early_stopping: 5
  metric_for_best_model: eval_accuracy
  greater_is_better: true
  load_best_model_at_end: true
  mixed_precision: bf16
  tf32: true
  seed: 42
  report_to:
  - wandb
optimizer:
  name: adamw
  lr: 2e-5
  weight_decay: 0.01
  betas:
  - 0.9
  - 0.999
  eps: 1e-8
scheduler:
  name: linear
  warmup_percent: 10
datacollator:
  pad_to_multiple_of: 8
wandb:
  project: neobert-glue
  name: neobert-100m-qnli-100k
  mode: online
