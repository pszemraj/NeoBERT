task: "pretraining"

model:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 512
  flash_attention: false  # Disabled due to version mismatch
  ngpt: false
  hidden_act: "gelu"  # Use GELU to avoid xformers issues

dataset:
  name: "pszemraj/simple_wikipedia_LM"
  path: null
  streaming: false
  num_proc: 4
  max_seq_length: 512
  train_split: "train[:5000]"  # Small subset for testing
  eval_split: "validation[:500]"

tokenizer:
  name: "bert-base-uncased"
  max_length: 512

datacollator:
  mlm_probability: 0.15

trainer:
  output_dir: "./test_outputs/gpu_test"
  run_name: "test_gpu_pretrain"
  max_steps: 100
  per_device_train_batch_size: 16  # Should fit on 8GB GPU
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  learning_rate: 1e-4
  warmup_steps: 10
  logging_steps: 10
  eval_steps: 50
  save_steps: 100
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: []
  fp16: false
  bf16: true  # Use bf16 on modern GPU

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01

scheduler:
  name: "cosine"
  warmup_steps: 10
  total_steps: 100

wandb:
  mode: "disabled"