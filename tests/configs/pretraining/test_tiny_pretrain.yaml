task: "pretraining"

model:
  hidden_size: 64
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 128
  dropout_prob: 0.0
  vocab_size: 30522
  max_position_embeddings: 128
  flash_attention: false
  ngpt: false
  hidden_act: "GELU"

dataset:
  name: "pszemraj/simple_wikipedia_LM"
  path: null
  tokenizer_name: "bert-base-uncased"
  streaming: false
  num_proc: 1
  num_workers: 0
  max_seq_length: 128
  train_split: "train[:100]"  # Very small subset for testing
  eval_split: "validation[:10]"

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 128

datacollator:
  mlm_probability: 0.15

trainer:
  output_dir: "./test_outputs"
  run_name: "test_tiny_pretrain"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  warmup_steps: 10
  logging_steps: 5
  eval_steps: 10
  save_steps: 20
  dataloader_num_workers: 0
  remove_unused_columns: false
  report_to: []
  use_cpu: true

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01

scheduler:
  name: "cosine"
  warmup_steps: 10
  total_steps: 50