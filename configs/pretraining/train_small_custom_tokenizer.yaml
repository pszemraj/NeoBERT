task: "pretraining"

model:
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4  # Changed from 8 to make head_dim=64 for flash attention compatibility
  intermediate_size: 1024
  dropout_prob: 0.0
  vocab_size: 32000  # Match the custom tokenizer
  max_position_embeddings: 512
  attn_backend: flash_attn_varlen
  kernel_backend: auto
  ngpt: false
  hidden_act: "swiglu"
  rope: true
  rms_norm: true

dataset:
  name: "pszemraj/simple_wikipedia_LM"
  path: "./tokenized_data/custom_tokenizer_test"
  streaming: false
  num_proc: 4
  num_workers: 0  # Avoid pickling issues
  max_seq_length: 512
  train_split: "train[:5000]"  # Small subset for testing
  eval_split: "validation[:500]"

tokenizer:
  name: "pszemraj/bytebpe-tokenizer-32k-mlm"
  vocab_size: 32000
  max_length: 512
  truncation: true

datacollator:
  mlm_probability: 0.15
  mask_all: false
  pad_to_multiple_of: 8

trainer:
  output_dir: "./outputs/custom_bytebpe-tokenizer-32k_test"
  max_steps: 200
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  enforce_full_packed_batches: true
  logging_steps: 10
  log_train_accuracy: false
  log_grad_norm: true
  log_weight_norms: true
  eval_steps: 100
  save_steps: 100
  mixed_precision: "bf16"  # Use bf16 on modern GPU
  masked_logits_only_loss: true
  gradient_checkpointing: false
  resume_from_checkpoint: false

optimizer:
  name: "adamw"
  lr: 5e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: "cosine"
  warmup_steps: 20
  total_steps: 200

wandb:
  enabled: false
  mode: "disabled"

# Additional settings
seed: 42
debug: true
