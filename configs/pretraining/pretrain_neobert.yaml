task: "pretraining"

model:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.0
  vocab_size: 30522
  max_position_embeddings: 4096  # Extended context length
  attn_backend: flash_attn_varlen
  kernel_backend: auto
  ngpt: false
  hidden_act: "swiglu"
  rope: true
  rms_norm: true

dataset:
  name: "wikipedia"  # Change to your dataset
  path: null
  config: "20220301.en"
  trust_remote_code: true
  streaming: true
  num_proc: 16
  num_workers: 4
  max_seq_length: 512
  train_split: "train[:1%]"  # Use small subset for testing
  eval_split: null  # Auto-detect validation/test split when available
  eval_samples: 2048  # Otherwise reserve first train samples for eval and skip from train

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 512

datacollator:
  mlm_probability: 0.15
  pad_to_multiple_of: 8

trainer:
  output_dir: "./outputs/neobert_pretrain"
  max_steps: 1000000
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  logging_steps: 100
  eval_steps: 10000
  save_steps: 10000
  mixed_precision: "bf16"  # Use bf16 on modern GPUs
  masked_logits_only_loss: true  # default/recommended; false=legacy full-logits ablation path
  gradient_checkpointing: false
  resume_from_checkpoint: false

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: "cosine"
  warmup_steps: 10000
  total_steps: 1000000

wandb:
  enabled: true
  project: "neobert-pretraining"
  name: "neobert_pretrain"
  mode: "online"

# Additional settings
seed: 42
