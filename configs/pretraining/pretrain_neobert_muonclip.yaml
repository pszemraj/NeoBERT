task: "pretraining"

model:
  hidden_size: 768
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  dropout_prob: 0.0
  vocab_size: 30522
  max_position_embeddings: 4096  # Extended context length
  flash_attention: true  # Now available with flash-attn 2.7.3
  ngpt: false
  hidden_act: "swiglu"  # Default activation (requires xformers)
  rope: true
  rms_norm: true

dataset:
  name: "wikipedia"  # Change to your dataset
  path: null
  streaming: true
  num_proc: 16
  num_workers: 4
  max_seq_length: 512
  train_split: "train[:1%]"  # Use small subset for testing
  eval_split: "train[99%:]"

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 512

datacollator:
  mlm_probability: 0.15
  pad_to_multiple_of: 8

trainer:
  output_dir: "./outputs/neobert_muonclip"
  run_name: "neobert_muonclip_experiment"
  max_steps: 1000000
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  warmup_steps: 10000
  logging_steps: 100
  eval_steps: 10000
  save_steps: 10000
  dataloader_num_workers: 4
  remove_unused_columns: false
  report_to: ["wandb"]  # Remove if you don't want logging
  bf16: true  # Use bf16 on modern GPUs
  gradient_checkpointing: false
  resume_from_checkpoint: false

# MuonClip optimizer configuration
optimizer:
  name: "muonclip"  # Use MuonClip instead of AdamW
  lr: 1e-4

  # Standard parameters (also used by Adam for 1D params)
  weight_decay: 0.01
  betas: [0.9, 0.95]
  eps: 1e-10

  # Muon-specific parameters
  muon_beta: 0.95  # Momentum coefficient for Muon
  muon_decay: 0.0  # Weight decay for 2D params (usually 0)
  ns_steps: 5  # Newton-Schulz iterations (3-9 recommended)

  # QK-Clipping parameters
  enable_clipping: true  # Enable attention stability clipping
  clipping_threshold: 50.0  # Conservative for encoders (decoder default: 100)
  clipping_alpha: 0.5  # Q/K scaling balance (0.5 = equal)
  clipping_warmup_steps: 0  # Steps before enabling clipping

  # Monitoring and debugging
  monitor_attention_entropy: true  # Track attention diversity
  detect_anomalies: false  # Enable for debugging (slow)
  log_max_logits: true  # Log maximum attention logits
  offload_hooks_to_cpu: true  # Save GPU memory
  enable_profiling: false  # Detailed timing info

scheduler:
  name: "cosine"
  warmup_steps: 10000
  total_steps: 1000000

wandb:
  project: "neobert-muonclip"
  mode: "online"
  log_interval: 100

# Additional settings
seed: 42
mixed_precision: "bf16"

# Notes on MuonClip hyperparameters:
# - clipping_threshold: Start conservative (50-100), decrease if attention explodes
# - ns_steps: More iterations = better orthogonalization but slower (5-7 optimal)
# - clipping_alpha: 0.5 balances Q and K equally, adjust if needed
# - muon_beta: Similar to AdamW beta1, controls momentum (0.9-0.98)
# - enable_clipping: Can disable for ablation studies of pure Muon