# Small test configuration for GPU validation

task: pretraining

# Dataset configuration
dataset:
  path: "tokenized_data/wikitext_small"
  streaming: false

# Override model size for quick testing
model:
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 1024
  xformers_attention: true

# Small dataset and training params
trainer:
  output_dir: "outputs/test_gpu"
  max_steps: 20
  gradient_accumulation_steps: 1
  mixed_precision: "bf16"
  save_steps: 10
  logging_steps: 5
  eval_steps: 100
  per_device_train_batch_size: 8
  dataloader_num_workers: 0

scheduler:
  name: "cosine"
  warmup_steps: 5
  total_steps: 20
  final_lr_ratio: 0.1

optimizer:
  name: "adamw"
  lr: 1e-4

wandb:
  resume: never
  name: null
  project: "neo-bert"
  entity: null
  tags: []
  dir: "logs/wandb"
  mode: disabled

seed: 42
