task: "contrastive"

model:
  hidden_size: 64
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 128
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 128
  flash_attention: false
  ngpt: false
  hidden_act: "GELU"

dataset:
  name: "ALLNLI"
  path: null
  max_seq_length: 128

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 128

trainer:
  output_dir: "./test_outputs"
  run_name: "test_tiny_contrastive"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  warmup_steps: 10
  logging_steps: 5
  eval_steps: 10
  save_steps: 20
  dataloader_num_workers: 0
  remove_unused_columns: false
  report_to: []
  use_cpu: true

contrastive:
  temperature: 0.05
  pooling: "avg"
  loss_type: "simcse"

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01

scheduler:
  name: "cosine"
  warmup_steps: 10
  total_steps: 50