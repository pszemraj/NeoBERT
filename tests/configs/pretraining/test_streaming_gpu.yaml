# Test configuration for streaming datasets on GPU

task: pretraining

# Model configuration (small model for testing)
model:
  vocab_size: 30522
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 1024
  hidden_act: "swiglu"  # Test SwiGLU on GPU
  dropout_prob: 0.0
  norm_eps: 1e-6
  max_position_embeddings: 512
  rope: true
  rms_norm: true
  xformers_attention: true  # Enable xFormers attention for GPU

# Tokenizer configuration
tokenizer:
  name: "bert-base-uncased"

# Dataset configuration
dataset:
  name: "wikitext"
  config: "wikitext-2-raw-v1"
  text_column: "text"
  streaming: true

# Data collator configuration
datacollator:
  mlm_probability: 0.15
  pad_to_multiple_of: 8
  mask_all: false
  pack_sequences: false
  max_length: 512

# Optimizer configuration
optimizer:
  name: "adamw"
  lr: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 0.01

# Scheduler configuration
scheduler:
  name: "cosine"
  warmup_steps: 100
  total_steps: 1000
  final_lr_ratio: 0.1

# Trainer configuration
trainer:
  max_steps: 100
  gradient_accumulation_steps: 1
  mixed_precision: "bf16"  # Use bf16 on RTX 4070
  output_dir: "outputs/test_streaming_gpu"
  save_total_limit: 2
  save_steps: 50
  logging_steps: 10
  eval_steps: 50
  disable_tqdm: false
  per_device_train_batch_size: 16
  dataloader_num_workers: 2

# Weights & Biases configuration
wandb:
  project: "neobert-test"
  dir: "logs/wandb"
  enabled: false

# Random seed
seed: 42
