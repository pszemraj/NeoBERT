# Pre-tokenized dataset test configuration
task: pretraining

model:
  hidden_size: 128
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 256
  max_position_embeddings: 512
  vocab_size: 30522  # BERT's vocab size
  rope: true
  rms_norm: true
  hidden_act: "swiglu"
  dropout_prob: 0.0
  attn_backend: flash_attn_varlen
  kernel_backend: auto

dataset:
  name: "pszemraj/simple_wikipedia_LM"
  streaming: false
  max_seq_length: 128
  num_workers: 0
  pre_tokenize: false  # This dataset is already tokenized

tokenizer:
  name: "bert-base-uncased"

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01

scheduler:
  name: "cosine"
  warmup_steps: 10

trainer:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  max_steps: 200
  save_steps: 200
  eval_steps: 200
  logging_steps: 10
  output_dir: "./outputs/pretokenized_test"
  mixed_precision: "bf16"

datacollator:
  mlm_probability: 0.2
  pad_to_multiple_of: 8

wandb:
  project: "neobert-test"
  name: "pretokenized-200steps-test"
  mode: "online"

seed: 42
