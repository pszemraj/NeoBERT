task: "pretraining"

model:
  hidden_size: 64
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 128
  dropout_prob: 0.0
  vocab_size: 30522
  max_position_embeddings: 128
  flash_attention: false
  ngpt: false
  hidden_act: "gelu"

# First tokenize the dataset:
# python scripts/pretraining/tokenize_dataset.py --train_samples 100 --output ./tokenized_data/test_tiny
dataset:
  name: "pszemraj/simple_wikipedia_LM"
  path: "./tokenized_data/test_tiny"  # Use pre-tokenized data
  streaming: false
  num_proc: 1
  num_workers: 0
  max_seq_length: 128
  train_split: "train"
  eval_split: "validation[:10]"

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 128

datacollator:
  mlm_probability: 0.15

trainer:
  output_dir: "./test_outputs"
  run_name: "test_tiny_pretrain"
  max_steps: 50  # Use max_steps instead of epochs for quick test
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  learning_rate: 1e-4
  warmup_steps: 10
  logging_steps: 5
  eval_steps: 25
  save_steps: 25
  dataloader_num_workers: 0
  remove_unused_columns: false
  report_to: []
  use_cpu: false  # Set to true if no GPU

optimizer:
  name: "adamw"
  lr: 1e-4
  weight_decay: 0.01

scheduler:
  name: "cosine"
  warmup_steps: 10
  total_steps: 50