task: "glue"

model:
  hidden_size: 64
  num_hidden_layers: 2
  num_attention_heads: 2
  intermediate_size: 128
  dropout_prob: 0.1
  vocab_size: 30522
  max_position_embeddings: 128
  attn_backend: sdpa
  ngpt: false
  hidden_act: "gelu"
  from_hub: false

dataset:
  name: "cola"
  max_seq_length: 128

tokenizer:
  name: "bert-base-uncased"
  vocab_size: 30522
  max_length: 128

trainer:
  output_dir: "./test_outputs"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 1
  logging_steps: 5
  eval_steps: 10
  save_steps: 20
  dataloader_num_workers: 0
  report_to: []
  use_cpu: true

glue:
  task_name: "cola"
  num_labels: 2
  pretrained_model_path: "tests/configs/pretraining/test_tiny_pretrain.yaml"
  classifier_dropout: 0.1
  classifier_init_range: 0.02
  transfer_from_task: false
  pretrained_checkpoint_dir: "./test_outputs"
  pretrained_checkpoint: 10

optimizer:
  name: "adamw"
  lr: 2e-5
  weight_decay: 0.01

scheduler:
  name: "linear"
  warmup_steps: 10
