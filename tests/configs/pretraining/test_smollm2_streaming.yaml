task: pretraining

dataset:
  name: "EleutherAI/SmolLM2-1.7B-stage-4-100B"
  train_split: "train"
  streaming: true
  shuffle_buffer_size: 10000
  num_workers: 0  # Required for streaming datasets
  max_seq_length: 512

model:
  vocab_size: 30522
  hidden_size: 512
  num_hidden_layers: 8
  num_attention_heads: 8
  intermediate_size: 2048
  max_position_embeddings: 512
  dropout_prob: 0.0
  xformers_attention: false
  rope: true
  rms_norm: true
  hidden_act: "swiglu"

tokenizer:
  path: "google-bert/bert-base-uncased"
  max_length: 512
  padding: "max_length"
  truncation: true

trainer:
  output_dir: "./outputs/smollm2_streaming_test"
  max_steps: 200
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  save_steps: 100
  logging_steps: 10
  mixed_precision: "bf16"
  gradient_checkpointing: false
  overwrite_output_dir: true
  report_to: ["wandb"]
  dataloader_num_workers: 0
  gradient_clipping: 1.0

optimizer:
  name: "adamw"
  lr: 5.0e-5
  weight_decay: 0.01
  eps: 1.0e-8
  betas: [0.9, 0.999]

scheduler:
  name: "cosine"
  warmup_steps: 20

datacollator:
  mlm_probability: 0.15
  pad_to_multiple_of: 8

wandb:
  enabled: true
  project: "neobert-streaming-test"
  name: "smollm2-streaming-200steps-v2"

seed: 42
